{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3727cd-2b1e-440b-8a4a-80313295144d",
   "metadata": {},
   "source": [
    "### Install BELA workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59364c46-1744-4646-b332-9b5ffad2e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MultiEL'...\n",
      "remote: Enumerating objects: 164, done.\u001b[K\n",
      "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
      "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
      "remote: Total 164 (delta 59), reused 112 (delta 30), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (164/164), 4.04 MiB | 7.93 MiB/s, done.\n",
      "Resolving deltas: 100% (59/59), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/PyThaiNLP/MultiEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd92862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multiel\n",
      "  Downloading MultiEL-0.5-py3-none-any.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting accelerate>=0.9.0\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fairscale in /usr/local/lib/python3.8/dist-packages (from multiel) (0.4.9)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from multiel) (5.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (from multiel) (4.21.0)\n",
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hydra-submitit-launcher\n",
      "  Downloading hydra_submitit_launcher-1.2.0-py3-none-any.whl (5.2 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf==3.20\n",
      "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: hydra-core in /usr/local/lib/python3.8/dist-packages (from multiel) (1.0.7)\n",
      "Collecting ujson\n",
      "  Downloading ujson-5.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from multiel) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from multiel) (0.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from multiel) (0.1.95)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.9.0->multiel) (1.12.1+cu113)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.9.0->multiel) (1.22.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.9.0->multiel) (5.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.9.0->multiel) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->multiel) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->multiel) (4.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->multiel) (2.28.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.8/dist-packages (from hydra-core->multiel) (4.8)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core->multiel) (5.9.0)\n",
      "Requirement already satisfied: omegaconf<2.1,>=2.0.5 in /usr/local/lib/python3.8/dist-packages (from hydra-core->multiel) (2.0.6)\n",
      "Collecting hydra-core\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting submitit>=1.3.3\n",
      "  Downloading submitit-1.5.1-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hydra-core\n",
      "  Downloading hydra_core-1.3.1-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.3.0-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.3.0.dev1-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.3.0.dev0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.4/151.4 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev5-py3-none-any.whl (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.5/150.5 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev4-py3-none-any.whl (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.5/150.5 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev3-py3-none-any.whl (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev2-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev1-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.2-py3-none-any.whl (147 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.4/147.4 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.2.3-py3-none-any.whl (27 kB)\n",
      "Collecting hydra-core\n",
      "  Downloading hydra_core-1.1.2.dev0-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.0-py3-none-any.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.6/144.6 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.0rc1-py3-none-any.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.6/144.6 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.0.dev7-py3-none-any.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of hydra-submitit-launcher to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting hydra-submitit-launcher\n",
      "  Downloading hydra_submitit_launcher-1.1.6-py3-none-any.whl (5.2 kB)\n",
      "  Downloading hydra_submitit_launcher-1.1.5-py3-none-any.whl (5.1 kB)\n",
      "  Downloading hydra_submitit_launcher-1.1.1-py3-none-any.whl (5.1 kB)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lightning-utilities>=0.8.0\n",
      "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->multiel) (2022.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->multiel) (2022.7.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers->multiel) (0.12.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->multiel) (3.8.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning->multiel) (45.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->accelerate>=0.9.0->multiel) (3.0.9)\n",
      "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from submitit>=1.3.3->hydra-submitit-launcher->multiel) (2.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->hydra-core->multiel) (3.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->multiel) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->multiel) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->multiel) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->multiel) (2.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (6.0.2)\n",
      "Installing collected packages: faiss-gpu, ujson, submitit, protobuf, h5py, lightning-utilities, torchmetrics, hydra-submitit-launcher, accelerate, pytorch-lightning, multiel\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.5\n",
      "    Uninstalling protobuf-3.19.5:\n",
      "      Successfully uninstalled protobuf-3.19.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
      "detectron2 0.6 requires hydra-core>=1.1, but you have hydra-core 1.0.7 which is incompatible.\n",
      "detectron2 0.6 requires omegaconf>=2.1, but you have omegaconf 2.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.24.1 faiss-gpu-1.7.2 h5py-3.10.0 hydra-submitit-launcher-1.1.1 lightning-utilities-0.10.0 multiel-0.5 protobuf-3.20.0 pytorch-lightning-2.1.2 submitit-1.5.1 torchmetrics-1.2.0 ujson-5.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install multiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1cfa923-8332-4c72-9af5-81050f73644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Collecting torch==1.12.1+cu116\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torch-1.12.1%2Bcu116-cp38-cp38-linux_x86_64.whl (1904.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 GB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting torchvision==0.13.1+cu116\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl (23.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchaudio==0.12.1 in /usr/local/lib/python3.8/dist-packages (0.12.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.12.1+cu116) (4.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.1+cu116) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.1+cu116) (9.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.1+cu116) (1.22.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1+cu116) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1+cu116) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1+cu116) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1+cu116) (2.1.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu113\n",
      "    Uninstalling torch-1.12.1+cu113:\n",
      "      Successfully uninstalled torch-1.12.1+cu113\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.13.1+cu113\n",
      "    Uninstalling torchvision-0.13.1+cu113:\n",
      "      Successfully uninstalled torchvision-0.13.1+cu113\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "detectron2 0.6 requires hydra-core>=1.1, but you have hydra-core 1.0.7 which is incompatible.\n",
      "detectron2 0.6 requires omegaconf>=2.1, but you have omegaconf 2.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.1+cu116 torchvision-0.13.1+cu116\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2143e3f5-642c-4835-bc6c-05727fdcaa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.3.1 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)\n",
      "Collecting hydra-core==1.3.2\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Collecting packaging\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf<2.4,>=2.2\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Collecting PyYAML>=5.1.0\n",
      "  Using cached PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
      "Collecting zipp>=3.1.0\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Installing collected packages: antlr4-python3-runtime, zipp, PyYAML, packaging, omegaconf, importlib-resources, hydra-core\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
      "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
      "      Removing file or directory /usr/local/bin/pygrun\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/antlr4/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/antlr4_python3_runtime-4.9.3.dist-info/\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.8.1\n",
      "    Uninstalling zipp-3.8.1:\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/__pycache__/zipp.cpython-38.pyc\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/zipp-3.8.1.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/zipp.py\n",
      "      Successfully uninstalled zipp-3.8.1\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/PyYAML-6.0.1.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/_yaml/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/yaml/\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/packaging-21.3.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/packaging/\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.3.0\n",
      "    Uninstalling omegaconf-2.3.0:\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/omegaconf-2.3.0.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/omegaconf/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/pydevd_plugins/\n",
      "      Successfully uninstalled omegaconf-2.3.0\n",
      "  Attempting uninstall: importlib-resources\n",
      "    Found existing installation: importlib-resources 5.9.0\n",
      "    Uninstalling importlib-resources-5.9.0:\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/importlib_resources-5.9.0.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/importlib_resources/\n",
      "      Successfully uninstalled importlib-resources-5.9.0\n",
      "  Attempting uninstall: hydra-core\n",
      "    Found existing installation: hydra-core 1.0.7\n",
      "    Uninstalling hydra-core-1.0.7:\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/hydra/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/hydra_core-1.0.7.dist-info/\n",
      "      Successfully uninstalled hydra-core-1.0.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fairseq 1.0.0a0+829d49a requires hydra-core<1.1, but you have hydra-core 1.3.2 which is incompatible.\n",
      "fairseq 1.0.0a0+829d49a requires omegaconf<2.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "sigopt 8.5.1 requires PyYAML<6.0.0,>=5.4.1, but you have pyyaml 6.0.1 which is incompatible.\n",
      "jupyterlab-server 2.16.3 requires importlib-metadata>=4.8.3; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
      "hydra-submitit-launcher 1.1.1 requires hydra-core==1.0.*, but you have hydra-core 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 importlib-resources-6.1.1 omegaconf-2.3.0 packaging-23.2 zipp-3.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --force-reinstall -v \"hydra-core==1.3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b8aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiel import BELA\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1054ed8-3c4a-4da8-912a-99ed52b14264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.3.1 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)\n",
      "Collecting omegaconf==2.3.0\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Running command python setup.py egg_info\n",
      "  running egg_info\n",
      "  creating /tmp/pip-pip-egg-info-mab_hn43/antlr4_python3_runtime.egg-info\n",
      "  writing /tmp/pip-pip-egg-info-mab_hn43/antlr4_python3_runtime.egg-info/PKG-INFO\n",
      "  writing dependency_links to /tmp/pip-pip-egg-info-mab_hn43/antlr4_python3_runtime.egg-info/dependency_links.txt\n",
      "  writing requirements to /tmp/pip-pip-egg-info-mab_hn43/antlr4_python3_runtime.egg-info/requires.txt\n",
      "  writing top-level names to /tmp/pip-pip-egg-info-mab_hn43/antlr4_python3_runtime.egg-info/top_level.txt\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-mab_hn43/antlr4_python3_runtime.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/tmp/pip-pip-egg-info-mab_hn43/antlr4_python3_runtime.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching '*.py' under directory 'test'\n",
      "  warning: no files found matching '*.c' under directory 'test'\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-mab_hn43/antlr4_python3_runtime.egg-info/SOURCES.txt'\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting PyYAML>=5.1.0\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.6/736.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Running command python setup.py bdist_wheel\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib\n",
      "  creating build/lib/antlr4\n",
      "  copying src/antlr4/LL1Analyzer.py -> build/lib/antlr4\n",
      "  copying src/antlr4/__init__.py -> build/lib/antlr4\n",
      "  copying src/antlr4/StdinStream.py -> build/lib/antlr4\n",
      "  copying src/antlr4/ListTokenSource.py -> build/lib/antlr4\n",
      "  copying src/antlr4/ParserInterpreter.py -> build/lib/antlr4\n",
      "  copying src/antlr4/RuleContext.py -> build/lib/antlr4\n",
      "  copying src/antlr4/Token.py -> build/lib/antlr4\n",
      "  copying src/antlr4/IntervalSet.py -> build/lib/antlr4\n",
      "  copying src/antlr4/Utils.py -> build/lib/antlr4\n",
      "  copying src/antlr4/InputStream.py -> build/lib/antlr4\n",
      "  copying src/antlr4/ParserRuleContext.py -> build/lib/antlr4\n",
      "  copying src/antlr4/Lexer.py -> build/lib/antlr4\n",
      "  copying src/antlr4/BufferedTokenStream.py -> build/lib/antlr4\n",
      "  copying src/antlr4/CommonTokenStream.py -> build/lib/antlr4\n",
      "  copying src/antlr4/FileStream.py -> build/lib/antlr4\n",
      "  copying src/antlr4/Recognizer.py -> build/lib/antlr4\n",
      "  copying src/antlr4/Parser.py -> build/lib/antlr4\n",
      "  copying src/antlr4/PredictionContext.py -> build/lib/antlr4\n",
      "  copying src/antlr4/TokenStreamRewriter.py -> build/lib/antlr4\n",
      "  copying src/antlr4/CommonTokenFactory.py -> build/lib/antlr4\n",
      "  creating build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/__init__.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/LexerActionExecutor.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/LexerAction.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/LexerATNSimulator.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/ATNState.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/ATNConfig.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/SemanticContext.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/PredictionMode.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/ATNDeserializer.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/ATNConfigSet.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/Transition.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/ATNDeserializationOptions.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/ATNType.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/ATN.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/ATNSimulator.py -> build/lib/antlr4/atn\n",
      "  copying src/antlr4/atn/ParserATNSimulator.py -> build/lib/antlr4/atn\n",
      "  creating build/lib/antlr4/dfa\n",
      "  copying src/antlr4/dfa/__init__.py -> build/lib/antlr4/dfa\n",
      "  copying src/antlr4/dfa/DFAState.py -> build/lib/antlr4/dfa\n",
      "  copying src/antlr4/dfa/DFASerializer.py -> build/lib/antlr4/dfa\n",
      "  copying src/antlr4/dfa/DFA.py -> build/lib/antlr4/dfa\n",
      "  creating build/lib/antlr4/tree\n",
      "  copying src/antlr4/tree/__init__.py -> build/lib/antlr4/tree\n",
      "  copying src/antlr4/tree/TokenTagToken.py -> build/lib/antlr4/tree\n",
      "  copying src/antlr4/tree/Tree.py -> build/lib/antlr4/tree\n",
      "  copying src/antlr4/tree/Chunk.py -> build/lib/antlr4/tree\n",
      "  copying src/antlr4/tree/Trees.py -> build/lib/antlr4/tree\n",
      "  copying src/antlr4/tree/RuleTagToken.py -> build/lib/antlr4/tree\n",
      "  copying src/antlr4/tree/ParseTreePattern.py -> build/lib/antlr4/tree\n",
      "  copying src/antlr4/tree/ParseTreeMatch.py -> build/lib/antlr4/tree\n",
      "  copying src/antlr4/tree/ParseTreePatternMatcher.py -> build/lib/antlr4/tree\n",
      "  creating build/lib/antlr4/error\n",
      "  copying src/antlr4/error/__init__.py -> build/lib/antlr4/error\n",
      "  copying src/antlr4/error/ErrorStrategy.py -> build/lib/antlr4/error\n",
      "  copying src/antlr4/error/ErrorListener.py -> build/lib/antlr4/error\n",
      "  copying src/antlr4/error/Errors.py -> build/lib/antlr4/error\n",
      "  copying src/antlr4/error/DiagnosticErrorListener.py -> build/lib/antlr4/error\n",
      "  creating build/lib/antlr4/xpath\n",
      "  copying src/antlr4/xpath/__init__.py -> build/lib/antlr4/xpath\n",
      "  copying src/antlr4/xpath/XPath.py -> build/lib/antlr4/xpath\n",
      "  running build_scripts\n",
      "  creating build/scripts-3.8\n",
      "  copying and adjusting bin/pygrun -> build/scripts-3.8\n",
      "  changing mode of build/scripts-3.8/pygrun from 644 to 755\n",
      "  installing to build/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build/bdist.linux-x86_64\n",
      "  creating build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/LL1Analyzer.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/__init__.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  creating build/bdist.linux-x86_64/wheel/antlr4/xpath\n",
      "  copying build/lib/antlr4/xpath/__init__.py -> build/bdist.linux-x86_64/wheel/antlr4/xpath\n",
      "  copying build/lib/antlr4/xpath/XPath.py -> build/bdist.linux-x86_64/wheel/antlr4/xpath\n",
      "  copying build/lib/antlr4/StdinStream.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/ListTokenSource.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/ParserInterpreter.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/RuleContext.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/Token.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/IntervalSet.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/Utils.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/InputStream.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/ParserRuleContext.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/Lexer.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/BufferedTokenStream.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/CommonTokenStream.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  creating build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/tree/__init__.py -> build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/tree/TokenTagToken.py -> build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/tree/Tree.py -> build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/tree/Chunk.py -> build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/tree/Trees.py -> build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/tree/RuleTagToken.py -> build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/tree/ParseTreePattern.py -> build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/tree/ParseTreeMatch.py -> build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/tree/ParseTreePatternMatcher.py -> build/bdist.linux-x86_64/wheel/antlr4/tree\n",
      "  copying build/lib/antlr4/FileStream.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  creating build/bdist.linux-x86_64/wheel/antlr4/dfa\n",
      "  copying build/lib/antlr4/dfa/__init__.py -> build/bdist.linux-x86_64/wheel/antlr4/dfa\n",
      "  copying build/lib/antlr4/dfa/DFAState.py -> build/bdist.linux-x86_64/wheel/antlr4/dfa\n",
      "  copying build/lib/antlr4/dfa/DFASerializer.py -> build/bdist.linux-x86_64/wheel/antlr4/dfa\n",
      "  copying build/lib/antlr4/dfa/DFA.py -> build/bdist.linux-x86_64/wheel/antlr4/dfa\n",
      "  copying build/lib/antlr4/Recognizer.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/Parser.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/PredictionContext.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  copying build/lib/antlr4/TokenStreamRewriter.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  creating build/bdist.linux-x86_64/wheel/antlr4/error\n",
      "  copying build/lib/antlr4/error/__init__.py -> build/bdist.linux-x86_64/wheel/antlr4/error\n",
      "  copying build/lib/antlr4/error/ErrorStrategy.py -> build/bdist.linux-x86_64/wheel/antlr4/error\n",
      "  copying build/lib/antlr4/error/ErrorListener.py -> build/bdist.linux-x86_64/wheel/antlr4/error\n",
      "  copying build/lib/antlr4/error/Errors.py -> build/bdist.linux-x86_64/wheel/antlr4/error\n",
      "  copying build/lib/antlr4/error/DiagnosticErrorListener.py -> build/bdist.linux-x86_64/wheel/antlr4/error\n",
      "  copying build/lib/antlr4/CommonTokenFactory.py -> build/bdist.linux-x86_64/wheel/antlr4\n",
      "  creating build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/__init__.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/LexerActionExecutor.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/LexerAction.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/LexerATNSimulator.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/ATNState.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/ATNConfig.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/SemanticContext.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/PredictionMode.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/ATNDeserializer.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/ATNConfigSet.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/Transition.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/ATNDeserializationOptions.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/ATNType.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/ATN.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/ATNSimulator.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  copying build/lib/antlr4/atn/ParserATNSimulator.py -> build/bdist.linux-x86_64/wheel/antlr4/atn\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  writing src/antlr4_python3_runtime.egg-info/PKG-INFO\n",
      "  writing dependency_links to src/antlr4_python3_runtime.egg-info/dependency_links.txt\n",
      "  writing requirements to src/antlr4_python3_runtime.egg-info/requires.txt\n",
      "  writing top-level names to src/antlr4_python3_runtime.egg-info/top_level.txt\n",
      "  reading manifest file 'src/antlr4_python3_runtime.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching '*.py' under directory 'test'\n",
      "  warning: no files found matching '*.c' under directory 'test'\n",
      "  writing manifest file 'src/antlr4_python3_runtime.egg-info/SOURCES.txt'\n",
      "  Copying src/antlr4_python3_runtime.egg-info to build/bdist.linux-x86_64/wheel/antlr4_python3_runtime-4.9.3.egg-info\n",
      "  running install_scripts\n",
      "  creating build/bdist.linux-x86_64/wheel/antlr4_python3_runtime-4.9.3.data\n",
      "  creating build/bdist.linux-x86_64/wheel/antlr4_python3_runtime-4.9.3.data/scripts\n",
      "  copying build/scripts-3.8/pygrun -> build/bdist.linux-x86_64/wheel/antlr4_python3_runtime-4.9.3.data/scripts\n",
      "  changing mode of build/bdist.linux-x86_64/wheel/antlr4_python3_runtime-4.9.3.data/scripts/pygrun to 755\n",
      "  creating build/bdist.linux-x86_64/wheel/antlr4_python3_runtime-4.9.3.dist-info/WHEEL\n",
      "  creating '/tmp/pip-wheel-6scz64u0/antlr4_python3_runtime-4.9.3-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "  adding 'antlr4/BufferedTokenStream.py'\n",
      "  adding 'antlr4/CommonTokenFactory.py'\n",
      "  adding 'antlr4/CommonTokenStream.py'\n",
      "  adding 'antlr4/FileStream.py'\n",
      "  adding 'antlr4/InputStream.py'\n",
      "  adding 'antlr4/IntervalSet.py'\n",
      "  adding 'antlr4/LL1Analyzer.py'\n",
      "  adding 'antlr4/Lexer.py'\n",
      "  adding 'antlr4/ListTokenSource.py'\n",
      "  adding 'antlr4/Parser.py'\n",
      "  adding 'antlr4/ParserInterpreter.py'\n",
      "  adding 'antlr4/ParserRuleContext.py'\n",
      "  adding 'antlr4/PredictionContext.py'\n",
      "  adding 'antlr4/Recognizer.py'\n",
      "  adding 'antlr4/RuleContext.py'\n",
      "  adding 'antlr4/StdinStream.py'\n",
      "  adding 'antlr4/Token.py'\n",
      "  adding 'antlr4/TokenStreamRewriter.py'\n",
      "  adding 'antlr4/Utils.py'\n",
      "  adding 'antlr4/__init__.py'\n",
      "  adding 'antlr4/atn/ATN.py'\n",
      "  adding 'antlr4/atn/ATNConfig.py'\n",
      "  adding 'antlr4/atn/ATNConfigSet.py'\n",
      "  adding 'antlr4/atn/ATNDeserializationOptions.py'\n",
      "  adding 'antlr4/atn/ATNDeserializer.py'\n",
      "  adding 'antlr4/atn/ATNSimulator.py'\n",
      "  adding 'antlr4/atn/ATNState.py'\n",
      "  adding 'antlr4/atn/ATNType.py'\n",
      "  adding 'antlr4/atn/LexerATNSimulator.py'\n",
      "  adding 'antlr4/atn/LexerAction.py'\n",
      "  adding 'antlr4/atn/LexerActionExecutor.py'\n",
      "  adding 'antlr4/atn/ParserATNSimulator.py'\n",
      "  adding 'antlr4/atn/PredictionMode.py'\n",
      "  adding 'antlr4/atn/SemanticContext.py'\n",
      "  adding 'antlr4/atn/Transition.py'\n",
      "  adding 'antlr4/atn/__init__.py'\n",
      "  adding 'antlr4/dfa/DFA.py'\n",
      "  adding 'antlr4/dfa/DFASerializer.py'\n",
      "  adding 'antlr4/dfa/DFAState.py'\n",
      "  adding 'antlr4/dfa/__init__.py'\n",
      "  adding 'antlr4/error/DiagnosticErrorListener.py'\n",
      "  adding 'antlr4/error/ErrorListener.py'\n",
      "  adding 'antlr4/error/ErrorStrategy.py'\n",
      "  adding 'antlr4/error/Errors.py'\n",
      "  adding 'antlr4/error/__init__.py'\n",
      "  adding 'antlr4/tree/Chunk.py'\n",
      "  adding 'antlr4/tree/ParseTreeMatch.py'\n",
      "  adding 'antlr4/tree/ParseTreePattern.py'\n",
      "  adding 'antlr4/tree/ParseTreePatternMatcher.py'\n",
      "  adding 'antlr4/tree/RuleTagToken.py'\n",
      "  adding 'antlr4/tree/TokenTagToken.py'\n",
      "  adding 'antlr4/tree/Tree.py'\n",
      "  adding 'antlr4/tree/Trees.py'\n",
      "  adding 'antlr4/tree/__init__.py'\n",
      "  adding 'antlr4/xpath/XPath.py'\n",
      "  adding 'antlr4/xpath/__init__.py'\n",
      "  adding 'antlr4_python3_runtime-4.9.3.data/scripts/pygrun'\n",
      "  adding 'antlr4_python3_runtime-4.9.3.dist-info/METADATA'\n",
      "  adding 'antlr4_python3_runtime-4.9.3.dist-info/WHEEL'\n",
      "  adding 'antlr4_python3_runtime-4.9.3.dist-info/top_level.txt'\n",
      "  adding 'antlr4_python3_runtime-4.9.3.dist-info/RECORD'\n",
      "  removing build/bdist.linux-x86_64/wheel\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=23467779f6243daf022d67cd9cbc6c7065788129d0bfe7146d75f07ddb491c6c\n",
      "  Stored in directory: /root/.cache/pip/wheels/11/20/e3/33bfd201db65084a9bb7ec0fd4040c2a7b59bb155698396a7c\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, PyYAML, omegaconf\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.8\n",
      "    Uninstalling antlr4-python3-runtime-4.8:\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/antlr4/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/antlr4_python3_runtime-4.8.dist-info/\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.8\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/PyYAML-5.4.1.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/_yaml/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/yaml/\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.0.6\n",
      "    Uninstalling omegaconf-2.0.6:\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/omegaconf-2.0.6.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.8/dist-packages/omegaconf/\n",
      "      Successfully uninstalled omegaconf-2.0.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fairseq 1.0.0a0+829d49a requires omegaconf<2.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "sigopt 8.5.1 requires PyYAML<6.0.0,>=5.4.1, but you have pyyaml 6.0.1 which is incompatible.\n",
      "hydra-core 1.0.7 requires antlr4-python3-runtime==4.8, but you have antlr4-python3-runtime 4.9.3 which is incompatible.\n",
      "hydra-core 1.0.7 requires omegaconf<2.1,>=2.0.5, but you have omegaconf 2.3.0 which is incompatible.\n",
      "detectron2 0.6 requires hydra-core>=1.1, but you have hydra-core 1.0.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --force-reinstall -v \"omegaconf==2.3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f3bc17-b9f6-4128-aeb2-a45fb1825ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from multiel import BELA\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
    "# !export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91954d5e-ac55-4409-b241-3826eddfbc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accelerate\n",
      "Version: 0.22.0\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: sylvain@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.8/dist-packages\n",
      "Requires: numpy, packaging, psutil, pyyaml, torch\n",
      "Required-by: MultiEL\n"
     ]
    }
   ],
   "source": [
    "! pip show accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b751d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/hydra/experimental/initialize.py:80: UserWarning: hydra.experimental.initialize_config_module() is no longer experimental. Use hydra.initialize_config_module().\n",
      "  deprecation_warning(message=message)\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/experimental/initialize.py:82: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize_config_module(\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'joint_el_mel_new': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'trainer/gpu_1_host': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'task/optim/adamw': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'task/model/xlmr_large': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'task/transform/joint_el_xlmr_raw_transform_large': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'datamodule/joint_el_datamodule': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'checkpoint_callback/default': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'task/joint_el_task': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#set intial threshold for NER and EL to 0.01 to increase amount of candidates\n",
    "bela_run = BELA(\n",
    " md_threshold=0.01,\n",
    " el_threshold=0.01, \n",
    " checkpoint_name =\"wiki\", \n",
    " # device = \"cuda:0\",\n",
    " config_name=\"joint_el_mel_new\",\n",
    " repo =\"wannaphong/BELA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c56b23",
   "metadata": {},
   "source": [
    "### WDSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c50e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wdsq = pd.read_csv('workspace/kbqa/kbqa/thesis/relation_ranking_our_data/alternative_datasets/lcquad_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01facb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>sparql</th>\n",
       "      <th>entity</th>\n",
       "      <th>relation</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_lbl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On Lake Winnipeg what is the lakes on river?</td>\n",
       "      <td>select distinct ?answer where { ?answer wdt:P4...</td>\n",
       "      <td>Q3272</td>\n",
       "      <td>P469</td>\n",
       "      <td>Q3292</td>\n",
       "      <td>Nelson River</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which is the PIM authority ID of Paul Erdős?</td>\n",
       "      <td>select distinct ?answer where { wd:Q173746 wdt...</td>\n",
       "      <td>Q173746</td>\n",
       "      <td>P3973</td>\n",
       "      <td>PIM52895</td>\n",
       "      <td>PIM52895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the MIA constituent ID for Johannes Gu...</td>\n",
       "      <td>select distinct ?answer where { wd:Q8958 wdt:P...</td>\n",
       "      <td>Q8958</td>\n",
       "      <td>P3603</td>\n",
       "      <td>12201</td>\n",
       "      <td>12201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is in the category of Percy Bysshe Shelley?</td>\n",
       "      <td>select distinct ?answer where { wd:Q93343 wdt:...</td>\n",
       "      <td>Q93343</td>\n",
       "      <td>P5506</td>\n",
       "      <td>percy-bysshe-shelley</td>\n",
       "      <td>percy-bysshe-shelley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the TDKIV ID for the blog?</td>\n",
       "      <td>select distinct ?answer where { wd:Q30849 wdt:...</td>\n",
       "      <td>Q30849</td>\n",
       "      <td>P5398</td>\n",
       "      <td>000014546</td>\n",
       "      <td>000014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>What is the CNO-11 occupation code for truck d...</td>\n",
       "      <td>select distinct ?answer where { wd:Q508846 wdt...</td>\n",
       "      <td>Q508846</td>\n",
       "      <td>P1022</td>\n",
       "      <td>8431, 8432</td>\n",
       "      <td>8431, 8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Which is the UNESCO Thesaurus ID for vandalism?</td>\n",
       "      <td>select distinct ?answer where { wd:Q6160 wdt:P...</td>\n",
       "      <td>Q6160</td>\n",
       "      <td>P3916</td>\n",
       "      <td>concept7160</td>\n",
       "      <td>concept7160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>Which is the Bulbapedia article of Super Smash...</td>\n",
       "      <td>select distinct ?answer where { wd:Q1052131 wd...</td>\n",
       "      <td>Q1052131</td>\n",
       "      <td>P4845</td>\n",
       "      <td>Super_Smash_Bros._Melee</td>\n",
       "      <td>Super_Smash_Bros._Melee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>What is  in the  CosIng number of citric acid ?</td>\n",
       "      <td>select distinct ?answer where { wd:Q159683 wdt...</td>\n",
       "      <td>Q159683</td>\n",
       "      <td>P3073</td>\n",
       "      <td>32858</td>\n",
       "      <td>32858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>What Theoi Project ID does Manticore has?</td>\n",
       "      <td>select distinct ?answer where { wd:Q223795 wdt...</td>\n",
       "      <td>Q223795</td>\n",
       "      <td>P3545</td>\n",
       "      <td>Thaumasios/Mantikhoras</td>\n",
       "      <td>Thaumasios/Mantikhoras</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>824 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0         On Lake Winnipeg what is the lakes on river?   \n",
       "1         Which is the PIM authority ID of Paul Erdős?   \n",
       "2    What is the MIA constituent ID for Johannes Gu...   \n",
       "3     What is in the category of Percy Bysshe Shelley?   \n",
       "4                   What is the TDKIV ID for the blog?   \n",
       "..                                                 ...   \n",
       "819  What is the CNO-11 occupation code for truck d...   \n",
       "820    Which is the UNESCO Thesaurus ID for vandalism?   \n",
       "821  Which is the Bulbapedia article of Super Smash...   \n",
       "822    What is  in the  CosIng number of citric acid ?   \n",
       "823          What Theoi Project ID does Manticore has?   \n",
       "\n",
       "                                                sparql    entity relation  \\\n",
       "0    select distinct ?answer where { ?answer wdt:P4...     Q3272     P469   \n",
       "1    select distinct ?answer where { wd:Q173746 wdt...   Q173746    P3973   \n",
       "2    select distinct ?answer where { wd:Q8958 wdt:P...     Q8958    P3603   \n",
       "3    select distinct ?answer where { wd:Q93343 wdt:...    Q93343    P5506   \n",
       "4    select distinct ?answer where { wd:Q30849 wdt:...    Q30849    P5398   \n",
       "..                                                 ...       ...      ...   \n",
       "819  select distinct ?answer where { wd:Q508846 wdt...   Q508846    P1022   \n",
       "820  select distinct ?answer where { wd:Q6160 wdt:P...     Q6160    P3916   \n",
       "821  select distinct ?answer where { wd:Q1052131 wd...  Q1052131    P4845   \n",
       "822  select distinct ?answer where { wd:Q159683 wdt...   Q159683    P3073   \n",
       "823  select distinct ?answer where { wd:Q223795 wdt...   Q223795    P3545   \n",
       "\n",
       "                      answer               answer_lbl  \n",
       "0                      Q3292             Nelson River  \n",
       "1                   PIM52895                 PIM52895  \n",
       "2                      12201                    12201  \n",
       "3       percy-bysshe-shelley     percy-bysshe-shelley  \n",
       "4                  000014546                000014546  \n",
       "..                       ...                      ...  \n",
       "819               8431, 8432               8431, 8432  \n",
       "820              concept7160              concept7160  \n",
       "821  Super_Smash_Bros._Melee  Super_Smash_Bros._Melee  \n",
       "822                    32858                    32858  \n",
       "823   Thaumasios/Mantikhoras   Thaumasios/Mantikhoras  \n",
       "\n",
       "[824 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wdsq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b061f3-976e-4435-ba18-e6cf2158b3b4",
   "metadata": {},
   "source": [
    "### 4 modes:\n",
    "1) 'bela_base' -- take initial question and rub BELA pipeline\n",
    "2) 'bela_large_case' -- take initial question, capitalize all words in the question and run BELA pipeline\n",
    "3) 'bela_large_ner' -- take initial question, capitalize just NER (use the code below) and run BELA pipeline\n",
    "4) 'bela_just_ner' -- take just NER from question (use the code below) and run BELA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe96322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wdsq.to_csv('workspace/kbqa/kbqa/thesis/relation_ranking_our_data/alternative_datasets/lcquad_simple.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba31a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [00:37<00:00, 22.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(df_wdsq))):\n",
    "#     df_wdsq.loc[i, 'bela_just_ner'] = bela_run.process_batch([df_wdsq.loc[i, 'ner_largecase']])\n",
    "#     df_wdsq.loc[i, 'bela_large_ner'] = bela_run.process_batch([df_wdsq.loc[i, 'ques_ner_largecase']])\n",
    "#     ques = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'].split(' ')])\n",
    "#     df_wdsq.loc[i, 'bela_large_case'] = bela_run.process_batch([ques])\n",
    "    df_wdsq.loc[i, 'bela_base'] = bela_run.process_batch([df_wdsq.loc[i, 'question']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5371982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [00:00<00:00, 3963.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#rerank bela entities by el scores\n",
    "for k in tqdm(range(len(df_wdsq))):\n",
    "    inds = np.argsort(df_wdsq.loc[k, 'bela_base'][0]['el_scores'])[::-1]\n",
    "    new_arr =[df_wdsq.loc[k, 'bela_base'][0]['entities'][i] for i in inds]\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_wdsq.loc[k, 'bela_base_ents'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "38caecdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_wdsq.query(\"bela_large_ner_ents == ''\"))/len(df_wdsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa54194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528\n",
      "3291\n",
      "5268\n",
      "5519\n"
     ]
    }
   ],
   "source": [
    "#if empty, take another prediction\n",
    "for i in range(len(df_wdsq)):\n",
    "    if df_wdsq.loc[i, \"bela_large_case_ents\"] == '':\n",
    "        print(i)\n",
    "        df_wdsq.loc[i, 'bela_large_case'] = df_wdsq.loc[i, 'bela_base']\n",
    "        df_wdsq.loc[i, 'bela_large_case_ents'] = df_wdsq.loc[i, 'bela_base_ents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c95f572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5622/5622 [00:01<00:00, 4588.45it/s]\n"
     ]
    }
   ],
   "source": [
    "#use 'bela_large_case' to retrieve 'ques_ner_largecase' and 'ner_largecase'\n",
    "\n",
    "for i in tqdm(range(len(df_wdsq))):\n",
    "    best_ent = np.argsort(df_wdsq.loc[i, 'bela_large_case'][0]['el_scores'])[::-1][0]\n",
    "    start = df_wdsq.loc[i, 'bela_large_case'][0]['offsets'][best_ent]\n",
    "    length = df_wdsq.loc[i, 'bela_large_case'][0]['lengths'][best_ent]\n",
    "    part = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'][start:start+length].split(' ')])\n",
    "    ques = df_wdsq.loc[i, 'question'][:start]+part+df_wdsq.loc[i, 'question'][start+length:]\n",
    "    df_wdsq.loc[i, 'ques_ner_largecase'] = ques\n",
    "    df_wdsq.loc[i, 'ner_largecase'] = part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b185a1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5622/5622 [00:00<00:00, 6049.48it/s]\n"
     ]
    }
   ],
   "source": [
    "#prepare questions with BELA NER for mGENRE\n",
    "\n",
    "for i in tqdm(range(len(df_wdsq))):\n",
    "    best_ent = np.argsort(df_wdsq.loc[i, 'bela_large_case'][0]['el_scores'])[::-1][0]\n",
    "    start = df_wdsq.loc[i, 'bela_large_case'][0]['offsets'][best_ent]\n",
    "    length = df_wdsq.loc[i, 'bela_large_case'][0]['lengths'][best_ent]\n",
    "    part = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'][start:start+length].split(' ')])\n",
    "    begin = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'][:start].split(' ')])\n",
    "    end = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'][start+length:].split(' ')])\n",
    "    ques = begin+'[START] '+part+' [END]'+end\n",
    "    df_wdsq.loc[i, 'mgenre_largecase_all'] = ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fca21473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_accuracy(df, col):\n",
    "    \n",
    "    count_1 = 0\n",
    "    count_2 = 0\n",
    "    count_3 = 0\n",
    "    count_4 = 0\n",
    "    count_5 = 0\n",
    "    count_6 = 0\n",
    "    for k in range(len(df)):\n",
    "        \n",
    "        if df.loc[k, col].split(', ')[0] == df.loc[k, 'subject']:\n",
    "            count_1 += 1\n",
    "            count_2 += 1\n",
    "            count_3 += 1\n",
    "            count_4 += 1\n",
    "            count_5 += 1\n",
    "            count_6 += 1\n",
    "        elif len(df.loc[k, col].split(', ')) >= 2 and df.loc[k, col].split(', ')[1] == df.loc[k, 'subject']:\n",
    "            count_2 += 1\n",
    "            count_3 += 1\n",
    "            count_4 += 1\n",
    "            count_5 += 1\n",
    "            count_6 += 1\n",
    "        elif len(df.loc[k, col].split(', ')) >= 3 and df.loc[k, col].split(', ')[2] == df.loc[k, 'subject']:\n",
    "            count_3 += 1\n",
    "            count_4 += 1\n",
    "            count_5 += 1\n",
    "            count_6 += 1   \n",
    "        elif len(df.loc[k, col].split(', ')) >= 4 and df.loc[k, col].split(', ')[3] == df.loc[k, 'subject']:\n",
    "            count_4 += 1\n",
    "            count_5 += 1\n",
    "            count_6 += 1   \n",
    "        elif len(df.loc[k, col].split(', ')) >= 5 and df.loc[k, col].split(', ')[4] == df.loc[k, 'subject']:\n",
    "            count_5 += 1\n",
    "            count_6 += 1\n",
    "        elif len(df.loc[k, col].split(', ')) >= 6 and df.loc[k, col].split(', ')[5] == df.loc[k, 'subject']:\n",
    "            count_6 += 1\n",
    "                \n",
    "    print('Top-1 accuracy:', count_1/len(df))\n",
    "    print('Top-2 accuracy:', count_2/len(df))\n",
    "    print('Top-3 accuracy:', count_3/len(df))\n",
    "    print('Top-4 accuracy:', count_4/len(df))\n",
    "    print('Top-5 accuracy:', count_5/len(df))\n",
    "    print('Top-6 accuracy:', count_6/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8514b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.5597652081109925\n",
      "Top-2 accuracy: 0.6561721807186055\n",
      "Top-3 accuracy: 0.6860547847741018\n",
      "Top-4 accuracy: 0.6997509782995376\n",
      "Top-5 accuracy: 0.7072216293134116\n",
      "Top-6 accuracy: 0.7127356812522234\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_wdsq, 'bela_base_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "69e191d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6463891853432941\n",
      "Top-2 accuracy: 0.7394165777303451\n",
      "Top-3 accuracy: 0.7689434364994664\n",
      "Top-4 accuracy: 0.7801494130202775\n",
      "Top-5 accuracy: 0.7858413376022768\n",
      "Top-6 accuracy: 0.7879758093205265\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_wdsq, 'bela_large_case_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dbbccc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6604411241551049\n",
      "Top-2 accuracy: 0.7427961579509071\n",
      "Top-3 accuracy: 0.7696549270722163\n",
      "Top-4 accuracy: 0.7822838847385272\n",
      "Top-5 accuracy: 0.7881536819637139\n",
      "Top-6 accuracy: 0.7911775168979011\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_wdsq, 'bela_large_ner_ents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a580e01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.5962290999644255\n",
      "Top-2 accuracy: 0.6536819637139808\n",
      "Top-3 accuracy: 0.6673781572394166\n",
      "Top-4 accuracy: 0.6725364638918534\n",
      "Top-5 accuracy: 0.675382426182853\n",
      "Top-6 accuracy: 0.6768054073283529\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_wdsq, 'bela_just_ner_ents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0b378801",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wdsq.to_csv('workspace/kbqa/kbqa/thesis/ent_linking_res/bela_for_mgenre_ques.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a37eb",
   "metadata": {},
   "source": [
    "### RuBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c3f6d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rubq = pd.read_csv('workspace/kbqa/kbqa/thesis/ent_linking_res/rubq_for_bela.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9be75a-b505-4728-9424-87a9b20886cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rubq = pd.read_csv('workspace/kbqa/kbqa/thesis/res_for_paper/rubq_paper_res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cadc332d-9e1b-4701-8f14-ce46d3e73d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rubq.to_csv('workspace/kbqa/kbqa/thesis/res_for_paper/rubq_paper_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b2ba-c2e5-456b-8a89-96c9fb31fff2",
   "metadata": {},
   "source": [
    "### swith between russian and enligsh translation of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cbaba7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [01:00<00:00, 28.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(df_rubq))):\n",
    "    df_rubq.loc[i, 'bela_just_ner'] = bela_run.process_batch([df_rubq.loc[i, 'ner_largecase']])\n",
    "#     df_rubq.loc[i, 'bela_large_ner'] = bela_run.process_batch([df_rubq.loc[i, 'ques_ner_largecase']])\n",
    "#     ques = ' '.join([elem.capitalize() for elem in df_rubq.loc[i, 'question_ru'].split(' ')])\n",
    "#     df_rubq.loc[i, 'bela_large_case_ru'] = bela_run.process_batch([ques])\n",
    "#     df_rubq.loc[i, 'bela_base_ru'] = bela_run.process_batch([df_rubq.loc[i, 'question_ru']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f3782a4-445f-4260-8b06-7d0a31218167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:57<00:00, 25.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(df_rubq))):\n",
    "        df_rubq.loc[i, 'bela_base'] = bela_run.process_batch([df_rubq.loc[i, 'question']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "29bea54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 8013.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    inds = np.argsort(df_rubq.loc[k, 'bela_just_ner'][0]['el_scores'])[::-1]\n",
    "    new_arr =[df_rubq.loc[k, 'bela_just_ner'][0]['entities'][i] for i in inds]\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_just_ner_ents'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7388aece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 4006.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#retrieve question with capitalized ner and ner\n",
    "\n",
    "for i in tqdm(range(len(df_rubq))):\n",
    "    best_ent = np.argsort(df_rubq.loc[i, 'bela_base'][0]['el_scores'])[::-1][0]\n",
    "    start = df_rubq.loc[i, 'bela_base'][0]['offsets'][best_ent]\n",
    "    length = df_rubq.loc[i, 'bela_base'][0]['lengths'][best_ent]\n",
    "    part = ' '.join([elem.capitalize() for elem in df_rubq.loc[i, 'question'][start:start+length].split(' ')])\n",
    "    ques = df_rubq.loc[i, 'question'][:start]+part+df_rubq.loc[i, 'question'][start+length:]\n",
    "    df_rubq.loc[i, 'ques_ner_largecase'] = ques\n",
    "    df_rubq.loc[i, 'ner_largecase'] = part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61885634-34e6-4cb3-9066-da4f84dc245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 56596.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#prepare questions with BELA NER for mGENRE\n",
    "\n",
    "for i in tqdm(range(len(df_rubq))):\n",
    "    if  pd.isna(df_rubq.loc[k, 'mgenre_with_bela_en_ques']):\n",
    "        \n",
    "        best_ent = np.argsort(df_rubq.loc[i, 'bela_base'][0]['el_scores'])[::-1][0]\n",
    "        start = df_rubq.loc[i, 'bela_base'][0]['offsets'][best_ent]\n",
    "        length = df_rubq.loc[i, 'bela_base'][0]['lengths'][best_ent]\n",
    "        part = ' '.join([elem for elem in df_rubq.loc[i, 'question'][start:start+length].split(' ')])\n",
    "        begin = ' '.join([elem for elem in df_rubq.loc[i, 'question'][:start].split(' ')])\n",
    "        end = ' '.join([elem for elem in df_rubq.loc[i, 'question'][start+length:].split(' ')])\n",
    "        ques = begin+'[START] '+part+' [END]'+end\n",
    "        df_rubq.loc[i, 'mgenre_with_bela_en_ques'] = ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b884e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_rubq.query(\"ner_largecase_ru == ''\"))/len(df_rubq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bda6b1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101\n"
     ]
    }
   ],
   "source": [
    "#if empty, take another prediction\n",
    "for i in range(len(df_rubq)):\n",
    "    if df_rubq.loc[i, \"bela_large_case_ents\"] == '':\n",
    "        print(i)\n",
    "        df_rubq.loc[i, 'bela_large_case'] = df_rubq.loc[i, 'bela_base']\n",
    "        df_rubq.loc[i, 'bela_large_case_ents'] = df_rubq.loc[i, 'bela_base_ents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9a01c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6824401368301026\n",
      "Top-2 accuracy: 0.7639680729760547\n",
      "Top-3 accuracy: 0.7833523375142531\n",
      "Top-4 accuracy: 0.7924743443557583\n",
      "Top-5 accuracy: 0.7953249714937286\n",
      "Top-6 accuracy: 0.7970353477765109\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_base_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "efb93e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6322690992018244\n",
      "Top-2 accuracy: 0.7246294184720639\n",
      "Top-3 accuracy: 0.7485746864310148\n",
      "Top-4 accuracy: 0.758266818700114\n",
      "Top-5 accuracy: 0.7605473204104903\n",
      "Top-6 accuracy: 0.7622576966932725\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_case_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "24c2a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6727480045610034\n",
      "Top-2 accuracy: 0.749714937286203\n",
      "Top-3 accuracy: 0.7730900798175598\n",
      "Top-4 accuracy: 0.7839224629418472\n",
      "Top-5 accuracy: 0.7873432155074116\n",
      "Top-6 accuracy: 0.7884834663625998\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_ner_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "773f0115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.5809578107183581\n",
      "Top-2 accuracy: 0.604903078677309\n",
      "Top-3 accuracy: 0.6077537058152793\n",
      "Top-4 accuracy: 0.6094640820980616\n",
      "Top-5 accuracy: 0.6100342075256556\n",
      "Top-6 accuracy: 0.6100342075256556\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_just_ner_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6514539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7696693272519954\n",
      "Top-2 accuracy: 0.838654503990878\n",
      "Top-3 accuracy: 0.8580387685290763\n",
      "Top-4 accuracy: 0.8700114025085519\n",
      "Top-5 accuracy: 0.8728620296465223\n",
      "Top-6 accuracy: 0.8779931584948689\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_base_ru_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c562bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7183580387685291\n",
      "Top-2 accuracy: 0.8055872291904219\n",
      "Top-3 accuracy: 0.8232611174458381\n",
      "Top-4 accuracy: 0.8352337514253135\n",
      "Top-5 accuracy: 0.8432155074116305\n",
      "Top-6 accuracy: 0.8449258836944128\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_case_ru_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b854fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.75769669327252\n",
      "Top-2 accuracy: 0.8204104903078677\n",
      "Top-3 accuracy: 0.8426453819840365\n",
      "Top-4 accuracy: 0.8557582668187002\n",
      "Top-5 accuracy: 0.8597491448118586\n",
      "Top-6 accuracy: 0.863740022805017\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_ner_ru_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "564b7375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6237172177879133\n",
      "Top-2 accuracy: 0.645381984036488\n",
      "Top-3 accuracy: 0.6510832383124288\n",
      "Top-4 accuracy: 0.6539338654503991\n",
      "Top-5 accuracy: 0.6545039908779932\n",
      "Top-6 accuracy: 0.6556442417331813\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_just_ner_ru_ents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e287ab",
   "metadata": {},
   "source": [
    "### Russian+English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0b5b3-b182-4a1d-bba2-f429478a409f",
   "metadata": {},
   "source": [
    "#### join Russian and English predictions for \"bela_base mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "06d0e2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 5752.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    \n",
    "    inds1 = df_rubq.loc[k, 'bela_base'][0]['el_scores']\n",
    "    inds2 = df_rubq.loc[k, 'bela_base_ru'][0]['el_scores']\n",
    "    inds = np.argsort(inds1+inds2)[::-1]\n",
    "\n",
    "    ents = df_rubq.loc[k, 'bela_base'][0]['entities']+df_rubq.loc[k, 'bela_base_ru'][0]['entities']\n",
    "\n",
    "    new_arr =[ents[i] for i in inds]\n",
    "\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_base_joined'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d543f-3854-4a78-a688-4209371a3b1a",
   "metadata": {},
   "source": [
    "#### join Russian and English predictions for \"bela_large_case mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a9c7df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 5775.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    \n",
    "    inds1 = df_rubq.loc[k, 'bela_large_case'][0]['el_scores']\n",
    "    inds2 = df_rubq.loc[k, 'bela_large_case_ru'][0]['el_scores']\n",
    "    inds = np.argsort(inds1+inds2)[::-1]\n",
    "\n",
    "    ents = df_rubq.loc[k, 'bela_large_case'][0]['entities']+df_rubq.loc[k, 'bela_large_case_ru'][0]['entities']\n",
    "\n",
    "    new_arr =[ents[i] for i in inds]\n",
    "\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_large_case_joined'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bcbbcb-6821-4af7-8d73-7bf82fd45508",
   "metadata": {},
   "source": [
    "#### join Russian and English predictions for \"bela_large_ner mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f676d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 5928.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    \n",
    "    inds1 = df_rubq.loc[k, 'bela_large_ner'][0]['el_scores']\n",
    "    inds2 = df_rubq.loc[k, 'bela_large_ner_ru'][0]['el_scores']\n",
    "    inds = np.argsort(inds1+inds2)[::-1]\n",
    "\n",
    "    ents = df_rubq.loc[k, 'bela_large_ner'][0]['entities']+df_rubq.loc[k, 'bela_large_ner_ru'][0]['entities']\n",
    "\n",
    "    new_arr =[ents[i] for i in inds]\n",
    "\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_large_ner_joined'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0629b00-cb90-4989-b368-aabc2ebdc3c6",
   "metadata": {},
   "source": [
    "#### join Russian and English predictions for \"bela_just_ner mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9ac96023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 7994.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    \n",
    "    inds1 = df_rubq.loc[k, 'bela_just_ner'][0]['el_scores']\n",
    "    inds2 = df_rubq.loc[k, 'bela_just_ner_ru'][0]['el_scores']\n",
    "    inds = np.argsort(inds1+inds2)[::-1]\n",
    "\n",
    "    ents = df_rubq.loc[k, 'bela_just_ner'][0]['entities']+df_rubq.loc[k, 'bela_just_ner_ru'][0]['entities']\n",
    "\n",
    "    new_arr =[ents[i] for i in inds]\n",
    "\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_just_ner_joined'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3a62667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7668187001140251\n",
      "Top-2 accuracy: 0.8574686431014823\n",
      "Top-3 accuracy: 0.8797035347776511\n",
      "Top-4 accuracy: 0.8928164196123147\n",
      "Top-5 accuracy: 0.8973774230330672\n",
      "Top-6 accuracy: 0.9007981755986317\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_base_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4f528d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7183580387685291\n",
      "Top-2 accuracy: 0.822690992018244\n",
      "Top-3 accuracy: 0.8517673888255416\n",
      "Top-4 accuracy: 0.8665906499429875\n",
      "Top-5 accuracy: 0.8734321550741163\n",
      "Top-6 accuracy: 0.878563283922463\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_case_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4d75003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7537058152793614\n",
      "Top-2 accuracy: 0.8472063854047891\n",
      "Top-3 accuracy: 0.8740022805017104\n",
      "Top-4 accuracy: 0.8825541619156214\n",
      "Top-5 accuracy: 0.8876852907639681\n",
      "Top-6 accuracy: 0.8916761687571265\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_ner_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4155fe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6470923603192702\n",
      "Top-2 accuracy: 0.7120866590649944\n",
      "Top-3 accuracy: 0.7200684150513113\n",
      "Top-4 accuracy: 0.7246294184720639\n",
      "Top-5 accuracy: 0.7274800456100342\n",
      "Top-6 accuracy: 0.7297605473204105\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_just_ner_joined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509bead-607b-497c-9a66-5aa76e524797",
   "metadata": {},
   "source": [
    "### Mintaka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83034da7-90bc-440c-a2b1-5d7a48c5cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mintaka = pd.read_csv('workspace/kbqa/kbqa/thesis/Relation_classification/data_mintaka/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f71a8634-0e0b-4326-900e-3fe88a6addaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:13<00:00, 25.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(df_mintaka))):\n",
    "        df_mintaka.loc[i, 'bela_base'] = bela_run.process_batch([df_mintaka.loc[i, 'question']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34fd771b-f609-4d5f-9cd6-2393b4970e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:00<00:00, 5363.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_mintaka))):\n",
    "    inds = np.argsort(df_mintaka.loc[k, 'bela_base'][0]['el_scores'])[::-1]\n",
    "    new_arr =[df_mintaka.loc[k, 'bela_base'][0]['entities'][i] for i in inds]\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_mintaka.loc[k, 'bela_base_ents'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e29c353a-0fe5-4cc9-b61a-88f272961297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:00<00:00, 4576.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#prepare questions with BELA NER for mGENRE\n",
    "\n",
    "for i in tqdm(range(len(df_mintaka))):        \n",
    "    best_ent = np.argsort(df_mintaka.loc[i, 'bela_base'][0]['el_scores'])[::-1][0]\n",
    "    start = df_mintaka.loc[i, 'bela_base'][0]['offsets'][best_ent]\n",
    "    length = df_mintaka.loc[i, 'bela_base'][0]['lengths'][best_ent]\n",
    "    part = ' '.join([elem for elem in df_mintaka.loc[i, 'question'][start:start+length].split(' ')])\n",
    "    begin = ' '.join([elem for elem in df_mintaka.loc[i, 'question'][:start].split(' ')])\n",
    "    end = ' '.join([elem for elem in df_mintaka.loc[i, 'question'][start+length:].split(' ')])\n",
    "    ques = begin+'[START] '+part+' [END]'+end\n",
    "    df_mintaka.loc[i, 'mgenre_with_bela_en_ques'] = ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be1a2058-d4d5-468b-a033-318c193adc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>entities</th>\n",
       "      <th>category</th>\n",
       "      <th>complexityType</th>\n",
       "      <th>answerType</th>\n",
       "      <th>answerLabel</th>\n",
       "      <th>questionType</th>\n",
       "      <th>ents_lbls</th>\n",
       "      <th>ents_len</th>\n",
       "      <th>bela_base</th>\n",
       "      <th>bela_base_ents</th>\n",
       "      <th>mgenre_with_bela_en_ques</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15aae099</td>\n",
       "      <td>Who is George Clooney's current wife?</td>\n",
       "      <td>[{'name': 'Q23844', 'entityType': 'entity', 'l...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q16769592</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q23844</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [0, 7, 7, 7, 7, 7, 7, 7, 14, 20, ...</td>\n",
       "      <td>Q23844, Q188830</td>\n",
       "      <td>Who is [START] George Clooney [END]'s current ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>da14605c</td>\n",
       "      <td>Who got the horse head in The Godfather?</td>\n",
       "      <td>[{'name': 'Q47703', 'entityType': 'entity', 'l...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q1159541</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q47703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [12, 12, 12, 18, 18, 26, 26, 26, ...</td>\n",
       "      <td>Q47703, Q925723, Q4996526, Q23640, Q726</td>\n",
       "      <td>Who got the horse head in [START] The Godfathe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4962bc3f</td>\n",
       "      <td>What movie won the Oscar for Best Picture in 1...</td>\n",
       "      <td>[{'name': 'Q102427', 'entityType': 'entity', '...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q202211</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q102427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [5, 19, 19, 19, 29, 34, 45], 'len...</td>\n",
       "      <td>Q102427, Q41918, Q753317, Q19020, Q11424</td>\n",
       "      <td>What movie won the [START] Oscar for Best Pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13eaddb4</td>\n",
       "      <td>Who won Best Actress at the Oscars in 2005?</td>\n",
       "      <td>[{'name': 'Q19020', 'entityType': 'entity', 'l...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q93187</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q19020, Q103618</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[{'offsets': [8, 8, 8, 8, 8, 8, 15, 28, 28, 28...</td>\n",
       "      <td>Q103618, Q19020, Q504962</td>\n",
       "      <td>Who won [START] Best Actress [END] at the Osca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32cd4730</td>\n",
       "      <td>Who is the actor that played John Kramer in Saw?</td>\n",
       "      <td>[{'name': 'Q12320195', 'entityType': 'entity',...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q310190</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q12320195, Q1145690</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[{'offsets': [11, 29, 29, 29, 29, 34, 38, 44, ...</td>\n",
       "      <td>Q486239, Q1985548, Q1145690, Q33999, Q193577, ...</td>\n",
       "      <td>Who is the actor that played John Kramer in [S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cb9379d4</td>\n",
       "      <td>Who was the famous actor from the Fast and Fur...</td>\n",
       "      <td>[{'name': 'Q1576873', 'entityType': 'entity', ...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q213864</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q1576873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [12, 19, 34, 34, 34, 34, 34, 39, ...</td>\n",
       "      <td>Q1576873, Q9687, Q33999, Q1420, Q1337738, Q171558</td>\n",
       "      <td>Who was the famous actor from the [START] Fast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a89fa7ea</td>\n",
       "      <td>Who is Emilio Estevez's father?</td>\n",
       "      <td>[{'name': 'Q220918', 'entityType': 'entity', '...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q184572</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q220918</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [7, 7, 7, 14, 18, 24], 'lengths':...</td>\n",
       "      <td>Q220918, Q7565</td>\n",
       "      <td>Who is [START] Emilio Estevez [END]'s father?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4ae2fbe1</td>\n",
       "      <td>Which Sinatra movie made in 1962 was remade in...</td>\n",
       "      <td>[{'name': 'Q42101', 'entityType': 'entity', 'l...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q521387</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q42101, Q40912</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[{'offsets': [0, 0, 0, 0, 6, 6, 6, 6, 6, 10, 1...</td>\n",
       "      <td>Q42101, Q40912, Q753339, Q643599, Q9199819, Q7...</td>\n",
       "      <td>Which Sinatra movie made in 1962 was remade in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56a1576a</td>\n",
       "      <td>Who stars in the Dirty Harry series?</td>\n",
       "      <td>[{'name': 'Q110206', 'entityType': 'entity', '...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q43203</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q110206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [17, 17, 17, 17, 20, 23], 'length...</td>\n",
       "      <td>Q2712648, Q60744426, Q73823</td>\n",
       "      <td>Who stars in the [START] Dirty Harry [END] ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>76f92061</td>\n",
       "      <td>What movie starred Mryna Loy and William Powell?</td>\n",
       "      <td>[{'name': None, 'entityType': 'entity', 'label...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q1198497</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q105960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [0, 5, 5, 15, 19, 19, 19, 19, 19,...</td>\n",
       "      <td>Q105941, Q105960, Q11424, Q96415185</td>\n",
       "      <td>What movie starred [START] Mryna Loy [END] and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cddb5d34</td>\n",
       "      <td>Which movie's plot concerns the fixing of game...</td>\n",
       "      <td>[{'name': 'Q11424', 'entityType': 'entity', 'l...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q651722</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q11424, Q846662</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[{'offsets': [0, 0, 0, 0, 6, 6, 6, 6, 11, 11, ...</td>\n",
       "      <td>Q846662, Q36297, Q11424, Q19, Q1044391, Q18597...</td>\n",
       "      <td>Which movie's plot concerns the fixing of [STA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ba1bcb28</td>\n",
       "      <td>Who is the oldest of the Baldwin brothers?</td>\n",
       "      <td>[{'name': 'Q2658216', 'entityType': 'entity', ...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q170572</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q2658216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [21, 25, 25, 25, 25, 25, 29, 33, ...</td>\n",
       "      <td>Q16147233, Q2658216, Q8923</td>\n",
       "      <td>Who is the oldest of the [START] Baldwin broth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15fe087a</td>\n",
       "      <td>What Academy Award did Tom Hanks receive in 1994?</td>\n",
       "      <td>[{'name': 'Q2263', 'entityType': 'entity', 'la...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q103916</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q2263, Q19020</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[{'offsets': [0, 5, 5, 5, 13, 23, 23, 30, 44],...</td>\n",
       "      <td>Q2263, Q19020, Q944352</td>\n",
       "      <td>What Academy Award did [START] Tom Hanks [END]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2d528e99</td>\n",
       "      <td>What is Jerry Seinfeld's wife's name?</td>\n",
       "      <td>[{'name': 'Q215506', 'entityType': 'entity', '...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q6187470</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q215506</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [0, 5, 8, 8, 8, 8, 8, 8, 14, 18, ...</td>\n",
       "      <td>Q215506, Q188830, Q82799</td>\n",
       "      <td>What is [START] Jerry Seinfeld [END]'s wife's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>be15fd94</td>\n",
       "      <td>Who played Ethan Edwards in The Searchers?</td>\n",
       "      <td>[{'name': None, 'entityType': 'entity', 'label...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q40531</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q276769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [11, 11, 11, 11, 11, 13, 17, 17, ...</td>\n",
       "      <td>Q276769, Q950345, Q7445790, Q113700, Q51422, Q...</td>\n",
       "      <td>Who played Ethan Edwards in [START] The Search...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bf6b3527</td>\n",
       "      <td>What was the first movie that Dwayne The Rock ...</td>\n",
       "      <td>[{'name': 'Q10738', 'entityType': 'entity', 'l...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q320461</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q10738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [13, 19, 19, 25, 30, 30, 30, 30, ...</td>\n",
       "      <td>Q10738, Q11424</td>\n",
       "      <td>What was the first movie that [START] Dwayne T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>79e96fbe</td>\n",
       "      <td>What is the name of character played by Kaley ...</td>\n",
       "      <td>[{'name': 'Q8539', 'entityType': 'entity', 'la...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q542063</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q8539, Q16759</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[{'offsets': [20, 20, 40, 40, 40, 44, 46, 49, ...</td>\n",
       "      <td>Q8539, Q16759, Q95074, Q15416</td>\n",
       "      <td>What is the name of character played by Kaley ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b7e48dba</td>\n",
       "      <td>What gang is Tony in from West Side Story?</td>\n",
       "      <td>[{'name': 'Q15238167', 'entityType': 'entity',...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q10666986</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q15238167, Q669010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[{'offsets': [0, 5, 5, 13, 13, 13, 26, 26, 26,...</td>\n",
       "      <td>Q669010, Q1049580, Q275186</td>\n",
       "      <td>What gang is Tony in from [START] West Side St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4b084e8a</td>\n",
       "      <td>Who won the Oscar for Best Actor in 2011?</td>\n",
       "      <td>[{'name': 'Q103916', 'entityType': 'entity', '...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q189422</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q103916</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [12, 12, 12, 12, 12, 22, 27, 36],...</td>\n",
       "      <td>Q103916, Q183355, Q19020</td>\n",
       "      <td>Who won the [START] Oscar for Best Actor [END]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4ac34e60</td>\n",
       "      <td>Who was the prisoner referenced in the title o...</td>\n",
       "      <td>[{'name': 'Q102448', 'entityType': 'entity', '...</td>\n",
       "      <td>movies</td>\n",
       "      <td>generic</td>\n",
       "      <td>entity</td>\n",
       "      <td>Q713701</td>\n",
       "      <td>simple</td>\n",
       "      <td>Q102448</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'offsets': [12, 12, 18, 39, 48, 48, 48, 48, ...</td>\n",
       "      <td>Q47598, Q1862087, Q715509, Q783521, Q8337</td>\n",
       "      <td>Who was the prisoner referenced in the title o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           question  \\\n",
       "0   15aae099              Who is George Clooney's current wife?   \n",
       "1   da14605c           Who got the horse head in The Godfather?   \n",
       "2   4962bc3f  What movie won the Oscar for Best Picture in 1...   \n",
       "3   13eaddb4        Who won Best Actress at the Oscars in 2005?   \n",
       "4   32cd4730   Who is the actor that played John Kramer in Saw?   \n",
       "5   cb9379d4  Who was the famous actor from the Fast and Fur...   \n",
       "6   a89fa7ea                    Who is Emilio Estevez's father?   \n",
       "7   4ae2fbe1  Which Sinatra movie made in 1962 was remade in...   \n",
       "8   56a1576a               Who stars in the Dirty Harry series?   \n",
       "9   76f92061   What movie starred Mryna Loy and William Powell?   \n",
       "10  cddb5d34  Which movie's plot concerns the fixing of game...   \n",
       "11  ba1bcb28         Who is the oldest of the Baldwin brothers?   \n",
       "12  15fe087a  What Academy Award did Tom Hanks receive in 1994?   \n",
       "13  2d528e99              What is Jerry Seinfeld's wife's name?   \n",
       "14  be15fd94         Who played Ethan Edwards in The Searchers?   \n",
       "15  bf6b3527  What was the first movie that Dwayne The Rock ...   \n",
       "16  79e96fbe  What is the name of character played by Kaley ...   \n",
       "17  b7e48dba         What gang is Tony in from West Side Story?   \n",
       "18  4b084e8a          Who won the Oscar for Best Actor in 2011?   \n",
       "19  4ac34e60  Who was the prisoner referenced in the title o...   \n",
       "\n",
       "                                             entities category complexityType  \\\n",
       "0   [{'name': 'Q23844', 'entityType': 'entity', 'l...   movies        generic   \n",
       "1   [{'name': 'Q47703', 'entityType': 'entity', 'l...   movies        generic   \n",
       "2   [{'name': 'Q102427', 'entityType': 'entity', '...   movies        generic   \n",
       "3   [{'name': 'Q19020', 'entityType': 'entity', 'l...   movies        generic   \n",
       "4   [{'name': 'Q12320195', 'entityType': 'entity',...   movies        generic   \n",
       "5   [{'name': 'Q1576873', 'entityType': 'entity', ...   movies        generic   \n",
       "6   [{'name': 'Q220918', 'entityType': 'entity', '...   movies        generic   \n",
       "7   [{'name': 'Q42101', 'entityType': 'entity', 'l...   movies        generic   \n",
       "8   [{'name': 'Q110206', 'entityType': 'entity', '...   movies        generic   \n",
       "9   [{'name': None, 'entityType': 'entity', 'label...   movies        generic   \n",
       "10  [{'name': 'Q11424', 'entityType': 'entity', 'l...   movies        generic   \n",
       "11  [{'name': 'Q2658216', 'entityType': 'entity', ...   movies        generic   \n",
       "12  [{'name': 'Q2263', 'entityType': 'entity', 'la...   movies        generic   \n",
       "13  [{'name': 'Q215506', 'entityType': 'entity', '...   movies        generic   \n",
       "14  [{'name': None, 'entityType': 'entity', 'label...   movies        generic   \n",
       "15  [{'name': 'Q10738', 'entityType': 'entity', 'l...   movies        generic   \n",
       "16  [{'name': 'Q8539', 'entityType': 'entity', 'la...   movies        generic   \n",
       "17  [{'name': 'Q15238167', 'entityType': 'entity',...   movies        generic   \n",
       "18  [{'name': 'Q103916', 'entityType': 'entity', '...   movies        generic   \n",
       "19  [{'name': 'Q102448', 'entityType': 'entity', '...   movies        generic   \n",
       "\n",
       "   answerType answerLabel questionType            ents_lbls  ents_len  \\\n",
       "0      entity   Q16769592       simple               Q23844       1.0   \n",
       "1      entity    Q1159541       simple               Q47703       1.0   \n",
       "2      entity     Q202211       simple              Q102427       1.0   \n",
       "3      entity      Q93187       simple      Q19020, Q103618       2.0   \n",
       "4      entity     Q310190       simple  Q12320195, Q1145690       2.0   \n",
       "5      entity     Q213864       simple             Q1576873       1.0   \n",
       "6      entity     Q184572       simple              Q220918       1.0   \n",
       "7      entity     Q521387       simple       Q42101, Q40912       2.0   \n",
       "8      entity      Q43203       simple              Q110206       1.0   \n",
       "9      entity    Q1198497       simple              Q105960       1.0   \n",
       "10     entity     Q651722       simple      Q11424, Q846662       2.0   \n",
       "11     entity     Q170572       simple             Q2658216       1.0   \n",
       "12     entity     Q103916       simple        Q2263, Q19020       2.0   \n",
       "13     entity    Q6187470       simple              Q215506       1.0   \n",
       "14     entity      Q40531       simple              Q276769       1.0   \n",
       "15     entity     Q320461       simple               Q10738       1.0   \n",
       "16     entity     Q542063       simple        Q8539, Q16759       2.0   \n",
       "17     entity   Q10666986       simple   Q15238167, Q669010       2.0   \n",
       "18     entity     Q189422       simple              Q103916       1.0   \n",
       "19     entity     Q713701       simple              Q102448       1.0   \n",
       "\n",
       "                                            bela_base  \\\n",
       "0   [{'offsets': [0, 7, 7, 7, 7, 7, 7, 7, 14, 20, ...   \n",
       "1   [{'offsets': [12, 12, 12, 18, 18, 26, 26, 26, ...   \n",
       "2   [{'offsets': [5, 19, 19, 19, 29, 34, 45], 'len...   \n",
       "3   [{'offsets': [8, 8, 8, 8, 8, 8, 15, 28, 28, 28...   \n",
       "4   [{'offsets': [11, 29, 29, 29, 29, 34, 38, 44, ...   \n",
       "5   [{'offsets': [12, 19, 34, 34, 34, 34, 34, 39, ...   \n",
       "6   [{'offsets': [7, 7, 7, 14, 18, 24], 'lengths':...   \n",
       "7   [{'offsets': [0, 0, 0, 0, 6, 6, 6, 6, 6, 10, 1...   \n",
       "8   [{'offsets': [17, 17, 17, 17, 20, 23], 'length...   \n",
       "9   [{'offsets': [0, 5, 5, 15, 19, 19, 19, 19, 19,...   \n",
       "10  [{'offsets': [0, 0, 0, 0, 6, 6, 6, 6, 11, 11, ...   \n",
       "11  [{'offsets': [21, 25, 25, 25, 25, 25, 29, 33, ...   \n",
       "12  [{'offsets': [0, 5, 5, 5, 13, 23, 23, 30, 44],...   \n",
       "13  [{'offsets': [0, 5, 8, 8, 8, 8, 8, 8, 14, 18, ...   \n",
       "14  [{'offsets': [11, 11, 11, 11, 11, 13, 17, 17, ...   \n",
       "15  [{'offsets': [13, 19, 19, 25, 30, 30, 30, 30, ...   \n",
       "16  [{'offsets': [20, 20, 40, 40, 40, 44, 46, 49, ...   \n",
       "17  [{'offsets': [0, 5, 5, 13, 13, 13, 26, 26, 26,...   \n",
       "18  [{'offsets': [12, 12, 12, 12, 12, 22, 27, 36],...   \n",
       "19  [{'offsets': [12, 12, 18, 39, 48, 48, 48, 48, ...   \n",
       "\n",
       "                                       bela_base_ents  \\\n",
       "0                                     Q23844, Q188830   \n",
       "1             Q47703, Q925723, Q4996526, Q23640, Q726   \n",
       "2            Q102427, Q41918, Q753317, Q19020, Q11424   \n",
       "3                            Q103618, Q19020, Q504962   \n",
       "4   Q486239, Q1985548, Q1145690, Q33999, Q193577, ...   \n",
       "5   Q1576873, Q9687, Q33999, Q1420, Q1337738, Q171558   \n",
       "6                                      Q220918, Q7565   \n",
       "7   Q42101, Q40912, Q753339, Q643599, Q9199819, Q7...   \n",
       "8                         Q2712648, Q60744426, Q73823   \n",
       "9                 Q105941, Q105960, Q11424, Q96415185   \n",
       "10  Q846662, Q36297, Q11424, Q19, Q1044391, Q18597...   \n",
       "11                         Q16147233, Q2658216, Q8923   \n",
       "12                             Q2263, Q19020, Q944352   \n",
       "13                           Q215506, Q188830, Q82799   \n",
       "14  Q276769, Q950345, Q7445790, Q113700, Q51422, Q...   \n",
       "15                                     Q10738, Q11424   \n",
       "16                      Q8539, Q16759, Q95074, Q15416   \n",
       "17                         Q669010, Q1049580, Q275186   \n",
       "18                           Q103916, Q183355, Q19020   \n",
       "19          Q47598, Q1862087, Q715509, Q783521, Q8337   \n",
       "\n",
       "                             mgenre_with_bela_en_ques  \n",
       "0   Who is [START] George Clooney [END]'s current ...  \n",
       "1   Who got the horse head in [START] The Godfathe...  \n",
       "2   What movie won the [START] Oscar for Best Pict...  \n",
       "3   Who won [START] Best Actress [END] at the Osca...  \n",
       "4   Who is the actor that played John Kramer in [S...  \n",
       "5   Who was the famous actor from the [START] Fast...  \n",
       "6       Who is [START] Emilio Estevez [END]'s father?  \n",
       "7   Which Sinatra movie made in 1962 was remade in...  \n",
       "8   Who stars in the [START] Dirty Harry [END] ser...  \n",
       "9   What movie starred [START] Mryna Loy [END] and...  \n",
       "10  Which movie's plot concerns the fixing of [STA...  \n",
       "11  Who is the oldest of the [START] Baldwin broth...  \n",
       "12  What Academy Award did [START] Tom Hanks [END]...  \n",
       "13  What is [START] Jerry Seinfeld [END]'s wife's ...  \n",
       "14  Who played Ethan Edwards in [START] The Search...  \n",
       "15  What was the first movie that [START] Dwayne T...  \n",
       "16  What is the name of character played by Kaley ...  \n",
       "17  What gang is Tony in from [START] West Side St...  \n",
       "18  Who won the [START] Oscar for Best Actor [END]...  \n",
       "19  Who was the prisoner referenced in the title o...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mintaka.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d41ab69-dd80-47bf-b272-cd61bd9cdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mintaka.to_csv('workspace/kbqa/kbqa/thesis/Relation_classification/data_mintaka/test_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
