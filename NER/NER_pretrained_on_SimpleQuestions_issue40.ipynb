{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359faa85",
   "metadata": {
    "id": "359faa85"
   },
   "source": [
    "Review of NER models is taken from here:\n",
    "https://arxiv.org/pdf/2205.00034.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73987784",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73987784",
    "outputId": "34965a24-a17e-4165-b413-c00d18d48d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting spark-nlp==4.0.2\n",
      "  Downloading spark_nlp-4.0.2-py2.py3-none-any.whl (532 kB)\n",
      "\u001b[K     |████████████████████████████████| 532 kB 5.1 MB/s \n",
      "\u001b[?25hCollecting pyspark==3.2.1\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 38 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 54.0 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=885e18953eb3c9d9ce427b4f15d55005770882ccad0ec6d220102dcec90020c3\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, spark-nlp, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1 spark-nlp-4.0.2\n"
     ]
    }
   ],
   "source": [
    "! pip install stanza\n",
    "! pip install spacy\n",
    "! pip install spark-nlp==4.0.2 pyspark==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4701de2",
   "metadata": {
    "id": "c4701de2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "import spacy\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04dec982",
   "metadata": {
    "id": "04dec982"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_train_marked.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd3618",
   "metadata": {
    "id": "1ddd3618"
   },
   "source": [
    "### Prediction by pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d8517",
   "metadata": {
    "id": "7c6d8517",
    "outputId": "7e73d5cc-af83-4dc1-b91e-8b8497c6cade"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 13:08:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 191kB [00:00, 8.71MB/s]                    \n",
      "2022-10-02 13:08:17 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-10-02 13:08:17 INFO: Use device: cpu\n",
      "2022-10-02 13:08:17 INFO: Loading: tokenize\n",
      "2022-10-02 13:08:17 INFO: Loading: ner\n",
      "2022-10-02 13:08:18 INFO: Done loading processors!\n",
      "100%|██████████| 19481/19481 [21:55<00:00, 14.81it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "\n",
    "#NER Stanza\n",
    "\n",
    "for j in tqdm(range(len(df))):\n",
    "    cur_question = df.loc[j, 'question']\n",
    "    doc = nlp(cur_question)\n",
    "    df.loc[j, 'ner_stanza'] =','.join([ent.text for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5918c5",
   "metadata": {
    "id": "be5918c5",
    "outputId": "554c7e14-4859-4218-fc35-9b5a88e33fcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19481/19481 [01:32<00:00, 209.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# NER spacy\n",
    "! python3 -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for j in tqdm(range(len(df))):\n",
    "    cur_question = df.loc[j, 'question']\n",
    "    doc = nlp(cur_question)\n",
    "    df.loc[j, 'ner_spacy'] =','.join([ent.text for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0756d63e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0756d63e",
    "outputId": "f618a1d1-dbc8-4f54-ac6d-9a7cd1dd663c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_dl download started this may take some time.\n",
      "Approx size to download 169.4 MB\n",
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19481/19481 [15:26<00:00, 21.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#NER SPARK\n",
    "\n",
    "#!NB this implementation works just in Colab\n",
    "# Start Spark Session with Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# Download a pre-trained pipeline\n",
    "pipeline = PretrainedPipeline('explain_document_dl', lang='en')\n",
    "\n",
    "for j in tqdm(range(len(df))):\n",
    "    cur_question = df.loc[j, 'question']\n",
    "    result = pipeline.annotate(cur_question)\n",
    "    df.loc[j, 'ner_spark'] =','.join(result['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ee2cc4",
   "metadata": {
    "id": "f1ee2cc4"
   },
   "outputs": [],
   "source": [
    "df.to_csv('train_ner_pretrained.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240c23d",
   "metadata": {
    "id": "1240c23d"
   },
   "source": [
    "### NER results (amount of rows where NER was not found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45acac43",
   "metadata": {
    "id": "45acac43"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_ner_pretrained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380240c4",
   "metadata": {
    "id": "380240c4",
    "outputId": "20235da2-62b4-45ae-b9ed-c46d7ad341dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7437503208254196\n"
     ]
    }
   ],
   "source": [
    "#NER Stanza\n",
    "count = 0\n",
    "for j in range(len(df)):\n",
    "    if pd.isna(df.loc[j, 'ner_stanza']):\n",
    "        count += 1\n",
    "print(count/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2543d7",
   "metadata": {
    "id": "9e2543d7",
    "outputId": "6a9dfea9-2f92-467e-88a3-c90ce35e66f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6418561675478671\n"
     ]
    }
   ],
   "source": [
    "#NER Spacy\n",
    "count = 0\n",
    "for j in range(len(df)):\n",
    "    if pd.isna(df.loc[j, 'ner_spacy']):\n",
    "        count += 1\n",
    "print(count/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53eb3f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a53eb3f0",
    "outputId": "ab4528a2-78bc-41dd-951f-b6e8fdef2e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8881474256968328\n"
     ]
    }
   ],
   "source": [
    "#NER SparklNLP\n",
    "count = 0\n",
    "for j in range(len(df)):\n",
    "    if pd.isna(df.loc[j, 'ner_spark']):\n",
    "        count += 1\n",
    "print(count/len(df))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
